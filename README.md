# GPT-2 Text Generator

This project implements an interactive text generation tool using OpenAI's GPT-2 model. It uses the Hugging Face Transformers library to load pre-trained GPT-2 models and generate human-like text continuations.

## ğŸ” Overview

- **Interactive Text Generation:** Enter any prompt and receive a coherent text continuation.
- **Model Size Selection:** Choose from GPT-2 small, medium, large, or xl.
- **Temperature Control:** Tune creativity/randomness in output.
- **Customizable Output:** Adjust text length and repetition constraints.

## âœ¨ Features

- ğŸ’¬ Real-time prompt interaction
- ğŸ›ï¸ Adjustable temperature (0.1 - 1.0)
- ğŸ§  Supports multiple model sizes
- âš™ï¸ Configurable output parameters

## ğŸ“¦ Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Jagdish918/GENERATIVE-TEXT-MODEL/tree/main
   cd gpt2-text-generator
![image](https://github.com/user-attachments/assets/a19407b4-e46f-4548-ad9c-0ad600a353eb)
